{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinnazim/esaa/blob/main/%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> - 확률적 경사 하강법\n",
        "- 미니배치 경사 하강법\n"
      ],
      "metadata": {
        "id": "yjAyGaiCMA01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 학습률이 높아져 알고리즘이 최적점에서 더 멀어지며 발산하고 있다. 학습률을 낮춰야한다."
      ],
      "metadata": {
        "id": "pJSMMQ0fMy-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 과소적합으로 높은 편향이 문제고 α를 감소 시켜야함"
      ],
      "metadata": {
        "id": "WhFUVtxNMPWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. 규제가 약간 있는 모델의 성능이 더 좋기 때문\n",
        "2. 라쏘 회귀는 가중치를 완전히 0으로 만드는 경향이 있기 때문\n",
        "3. 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 불규칙하게 행동하는 라쏘보다 엘라스틱넷이 나음"
      ],
      "metadata": {
        "id": "jEvxW9i4MchW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 데이터 준비\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# iris 데이터셋 불러오기\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이\n",
        "y = iris[\"target\"]"
      ],
      "metadata": {
        "id": "jOpgn51KT9nK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 샘플에 편향 추가\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "# 랜덤 시드 지정\n",
        "np.random.seed(2042)"
      ],
      "metadata": {
        "id": "jTYYwRDeULBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터 셋 분할\n",
        "test_ratio = 0.2                                         # 테스트 세트 비율 = 20%\n",
        "validation_ratio = 0.2                                   # 검증 세트 비율 = 20%\n",
        "total_size = len(X_with_bias)                            # 전체 데이터셋 크기\n",
        "\n",
        "test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%\n",
        "validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%\n",
        "train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%"
      ],
      "metadata": {
        "id": "rc44_WnjUZUf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_indices = np.random.permutation(total_size)"
      ],
      "metadata": {
        "id": "26YdupnTUadw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "M4X-A2UHUbp7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 타겟 변환\n",
        "\n",
        "# 학습을 위해 원-핫 벡터로 변환 필요-> 소프트 맥스 회귀는 각 클래스 별로 속할 확률을 구해 비용함수를 계산해서\n",
        "\n",
        "\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1                 # 클래스 수\n",
        "    m = len(y)                              # 샘플 수\n",
        "    Y_one_hot = np.zeros((m, n_classes))    # (샘플 수, 클래스 수) 0-벡터 생성\n",
        "    Y_one_hot[np.arange(m), y] = 1          # 샘플 별로 해당 클래스의 값만 1로 변경. (넘파이 인덱싱 활용)\n",
        "    return Y_one_hot\n",
        "\n",
        "\n",
        "to_one_hot(y_train[:5])\n",
        "\n",
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "sVw52OGAUd4h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 소프트맥스 함수 구현\n",
        "def softmax(logits):\n",
        "    exps = np.exp(logits)                            # 항목별 지수함수 적용\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)   # 샘플별 클래스 점수 합산\n",
        "    return exps / exp_sums                           # 샘플별 소프트맥스 점수로 이루어진 어레이 반환"
      ],
      "metadata": {
        "id": "JFCebV8vUjoM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경사하강법 구현을 위해 비용함수, 그레이이디언트 구현\n",
        "\n",
        "n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1\n",
        "n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)"
      ],
      "metadata": {
        "id": "EgSmImOkUpat"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  배치 경사하강법 구현\n",
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "for iteration in range(n_iterations):     # 5001번 반복 훈련\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "\n",
        "    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력\n",
        "        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        print(iteration, loss)\n",
        "\n",
        "    error = Y_proba - Y_train_one_hot     # 그레이디언트 계산.\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "\n",
        "    Theta = Theta - eta * gradients       # 파라미터 업데이트"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd2Uk7XzUsnk",
        "outputId": "a9dcbba6-8975-4454-9a6b-94e4394bdef4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 5.446205811872683\n",
            "500 0.8350062641405651\n",
            "1000 0.6878801447192402\n",
            "1500 0.6012379137693314\n",
            "2000 0.5444496861981872\n",
            "2500 0.5038530181431525\n",
            "3000 0.4729228972192248\n",
            "3500 0.44824244188957774\n",
            "4000 0.4278651093928793\n",
            "4500 0.41060071429187134\n",
            "5000 0.3956780375390374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6uB181bUuYu",
        "outputId": "80b0c04b-304e-425c-cdfe-da50ff45142a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.32094157, -0.6501102 , -2.99979416],\n",
              "       [-1.1718465 ,  0.11706172,  0.10507543],\n",
              "       [-0.70224261, -0.09527802,  1.4786383 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)          # 가장 높은 확률을 갖는 클래스 선택\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PbC3sYvUvXY",
        "outputId": "29a09224-25c8-4bc0-ab4c-c468d3e61f3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 규제 추가 경사하강법 활용 훈련(L2 규제 )\n",
        "\n",
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1        # 규제 하이퍼파라미터\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "\n",
        "    if iteration % 500 == 0:\n",
        "        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외\n",
        "        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실\n",
        "        print(iteration, loss)\n",
        "\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트\n",
        "    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients\n",
        "\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT5aMqJcU0oT",
        "outputId": "0ce54c0e-388f-4ef9-939f-a15c12d36c2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 6.629842469083912\n",
            "500 0.5339667976629505\n",
            "1000 0.503640075014894\n",
            "1500 0.4946891059460322\n",
            "2000 0.4912968418075477\n",
            "2500 0.48989924700933296\n",
            "3000 0.4892990598451198\n",
            "3500 0.48903512443978603\n",
            "4000 0.4889173621830818\n",
            "4500 0.4888643337449303\n",
            "5000 0.4888403120738818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9WF7x1CU61q",
        "outputId": "a0ee64c8-b084-44ef-b860-ab0ac50d4600"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. 조기 종료 추가\n",
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1            # 규제 하이퍼파라미터\n",
        "best_loss = np.infty   # 최소 손실값 기억 변수\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # 훈련 및 손실 계산\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    # 검증 세트에 대한 손실 계산\n",
        "    logits = X_valid.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "\n",
        "    # 500 에포크마다 검증 세트에 대한 손실 출력\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "\n",
        "    # 에포크마다 최소 손실값 업데이트\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료\n",
        "        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력\n",
        "        print(iteration, loss, \"조기 종료!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2AsEG_ZU7lE",
        "outputId": "be5529f1-fb75-47e2-de97-36c215a0959f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4.7096017363419875\n",
            "500 0.5739711987633519\n",
            "1000 0.5435638529109128\n",
            "1500 0.5355752782580261\n",
            "2000 0.5331959249285544\n",
            "2500 0.5325946767399383\n",
            "2765 0.5325460966791898\n",
            "2766 0.5325460971327977 조기 종료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCdJqgqcVBPp",
        "outputId": "6d4ab15a-adb1-4637-df44-9079f26ffbf9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}